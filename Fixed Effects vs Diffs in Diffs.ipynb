{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed effects and first differencing versus diffs in diffs\n",
    "Have always been a little confused as to whether to use diffs in diffs or fixed effects when we have panel data with two time periods.  In this notebook, I generate some fake data to simulate the effect of using one versus the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.98791903],\n",
       "       [ 0.98791903,  1.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate mock data\n",
    "sampsize = 1000\n",
    "pre = np.random.normal(0,1, sampsize)\n",
    "treat = np.random.binomial(1,.5,sampsize)\n",
    "# add a little bit of extra noise just to the \"treated\" units to\n",
    "# make them a bit different from the control units.  note that this would not be the case in an RCT\n",
    "# pre = pre + treat*np.random.normal(.1,.05,sampsize)\n",
    "\n",
    "# generate noise to add to the difference between pre and post\n",
    "# this is so that even without the treatment effect there would not be perfect correlation between pre and post\n",
    "noise = np.random.normal(0,.15,sampsize)\n",
    "effect = .1\n",
    "post = pre + noise + effect*treat\n",
    "\n",
    "# show the correlation between pre and post\n",
    "np.corrcoef(pre,post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015534681524\n",
      "0.00751526976216\n",
      "-0.0414956251501\n",
      "0.101513919793\n"
     ]
    }
   ],
   "source": [
    "# calculate the diffs in diffs estimate directly\n",
    "post_treat =post[treat==1].mean()\n",
    "pre_treat = pre[treat==1].mean()\n",
    "post_control =post[treat==0].mean()\n",
    "pre_control = pre[treat == 0].mean()\n",
    "\n",
    "# calculate the coeffs from a diffs in diffs model\n",
    "# note that these should match the coeffs from the model below\n",
    "did = (post_treat-pre_treat)-(post_control-pre_control)\n",
    "print(pre_control)\n",
    "print(post_control-pre_control)\n",
    "print(pre_treat-pre_control)\n",
    "print(did)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.001\n",
      "Model:                            OLS   Adj. R-squared:                 -0.000\n",
      "Method:                 Least Squares   F-statistic:                    0.9135\n",
      "Date:                Thu, 16 Feb 2017   Prob (F-statistic):              0.434\n",
      "Time:                        10:36:32   Log-Likelihood:                -2865.1\n",
      "No. Observations:                2000   AIC:                             5738.\n",
      "Df Residuals:                    1996   BIC:                             5761.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0155      0.044      0.354      0.723        -0.070     0.102\n",
      "x1             0.0075      0.062      0.121      0.904        -0.114     0.129\n",
      "x2            -0.0415      0.064     -0.645      0.519        -0.168     0.085\n",
      "x3             0.1015      0.091      1.116      0.265        -0.077     0.280\n",
      "==============================================================================\n",
      "Omnibus:                        7.322   Durbin-Watson:                   2.039\n",
      "Prob(Omnibus):                  0.026   Jarque-Bera (JB):                6.861\n",
      "Skew:                           0.105   Prob(JB):                       0.0324\n",
      "Kurtosis:                       2.805   Cond. No.                         6.66\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# estimate the effect using a diffs-in-diffs model\n",
    "outcome = np.concatenate((pre,post),0)\n",
    "repeat_treat = np.concatenate((treat,treat),0)\n",
    "round = np.concatenate((np.repeat(0,sampsize),np.repeat(1,sampsize)),0)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "X = np.stack((round, repeat_treat, round*repeat_treat),1)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "model = sm.OLS(outcome, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytical results for variance from DiD model\n",
    "For the DiD model above, my hunch was that the estimate of the variance would be equal to the sum of the variance of the four terms calculated above.  To make absolutely sure that this is the case, I calculated the the term $(X'X)^{-1}$ (including the intercept) to see how the variance is calculated for the regression. This is fairly easy in this case since the elements of X are all vectors with 1s and 0s.  (To test this out quickly, perform this operation on the X matrix above.) Sure enough, the diagonal term in this matrix corresponding to the treat_post variable is 4.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.103\n",
      "Model:                            OLS   Adj. R-squared:                  0.102\n",
      "Method:                 Least Squares   F-statistic:                     114.8\n",
      "Date:                Thu, 16 Feb 2017   Prob (F-statistic):           2.00e-25\n",
      "Time:                        10:36:32   Log-Likelihood:                 482.84\n",
      "No. Observations:                1000   AIC:                            -961.7\n",
      "Df Residuals:                     998   BIC:                            -951.9\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0075      0.006      1.163      0.245        -0.005     0.020\n",
      "x1             0.1015      0.009     10.713      0.000         0.083     0.120\n",
      "==============================================================================\n",
      "Omnibus:                        7.434   Durbin-Watson:                   1.947\n",
      "Prob(Omnibus):                  0.024   Jarque-Bera (JB):                8.991\n",
      "Skew:                          -0.101   Prob(JB):                       0.0112\n",
      "Kurtosis:                       3.418   Cond. No.                         2.54\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Estimate the same model using a first difference approach\n",
    "delta_y = post - pre\n",
    "delta_treat = treat\n",
    "X = delta_treat\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(delta_y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Results\n",
    "For the very simple case with no covariates presented above, the estimates of the impact are always the same but...\n",
    "1. The estimates of impact are always the same.\n",
    "2. When there is no difference in treat and control in the pre period (as you would expect in an RCT), the estimate of the standard errors of the estimators are pretty similar\n",
    "3. If there is a systematic difference between treat and control, the standard error of the first difference estimator can be much smaller than the standard error of the diffs in diffs estimator\n",
    "\n",
    "A couple of other things to consider\n",
    "\n",
    "1. With fixed effects / first differencing, you can't include covariates that don't change over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More stuff to do\n",
    "1. Ideally, I should derive the estimates of variance for the different estimators and see how they differ\n",
    "2. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
